{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, trim, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql.functions import countDistinct \n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter \n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela Clientes\n",
    "df_clientes = spark.sql(\"select * from desafio_curso.tbl_clientes\")\n",
    "# Tabela Divisao\n",
    "df_divisao = spark.sql(\"select * from desafio_curso.tbl_divisao\")\n",
    "# Tabela Endereco\n",
    "df_endereco = spark.sql(\"select * from desafio_curso.tbl_endereco\")\n",
    "# Tabela Regiao\n",
    "df_regiao = spark.sql(\"select * from desafio_curso.tbl_regiao\")\n",
    "# Tabela Vendas\n",
    "df_vendas = spark.sql(\"select * from desafio_curso.tbl_vendas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformação Dados\n",
    "\n",
    "df_endereco = df_endereco.withColumn('city', when(col('city').isNull() | (trim(col('city')) == ''), 'Não informado').otherwise(col('city'))) \\\n",
    "                         .withColumn('customer_address_1', when(col('customer_address_1').isNull() | (trim(col('customer_address_1')) == ''), 'Não informado').otherwise(col('customer_address_1'))) \\\n",
    "                         .withColumn('state', when(col('state').isNull() | (trim(col('state')) == ''), 'Não informado').otherwise(col('state'))) \\\n",
    "                         .withColumn('zip_code', when(col('zip_code').isNull() | (trim(col('zip_code')) == ''), 'Não informado').otherwise(col('zip_code')))\n",
    "\n",
    "df_vendas = df_vendas.withColumn('discount_amount', when(col('discount_amount').isNull() | (trim(col('discount_amount')) == ''), '0').otherwise(col('discount_amount'))) \\\n",
    "                     .withColumn('item_number', when(col('item_number').isNull() | (trim(col('item_number')) == ''), '0').otherwise(col('item_number'))) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterar tipo\n",
    "\n",
    "df_vendas = df_vendas.withColumn(\"discount_amount\", regexp_replace(col(\"discount_amount\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"list_price\", regexp_replace(col(\"list_price\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"sales_amount\", regexp_replace(col(\"sales_amount\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"sales_amount_based_on_list_price\", regexp_replace(col(\"sales_amount_based_on_list_price\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"sales_cost_amount\", regexp_replace(col(\"sales_cost_amount\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"sales_margin_amount\", regexp_replace(col(\"sales_margin_amount\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                     .withColumn(\"sales_price\", regexp_replace(col(\"sales_price\"), \",\", \".\").cast(DoubleType()))\n",
    "\n",
    " \n",
    "#Data\n",
    "df_vendas = df_vendas.withColumn(\"actual_delivery_date\", to_date(col(\"actual_delivery_date\"), \"dd/MM/yyyy\").cast(DateType())) \\\n",
    "                     .withColumn(\"datekey\", to_date(col(\"datekey\"), \"dd/MM/yyyy\").cast(DateType())) \\\n",
    "                     .withColumn(\"invoice_date\", to_date(col(\"invoice_date\"), \"dd/MM/yyyy\").cast(DateType())) \\\n",
    "                     .withColumn(\"promised_delivery_date\", to_date(col(\"promised_delivery_date\"), \"dd/MM/yyyy\").cast(DateType()))\n",
    "\n",
    "#Inteiro\n",
    "df_vendas = df_vendas.withColumn(\"item_number\",col(\"item_number\").cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes.createOrReplaceTempView('clientes')\n",
    "df_divisao.createOrReplaceTempView('divisao')\n",
    "df_endereco.createOrReplaceTempView('endereco')\n",
    "df_regiao.createOrReplaceTempView('regiao')\n",
    "df_vendas.createOrReplaceTempView('vendas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select\n",
    "a.customerkey,\n",
    "a.discount_amount,\n",
    "a.invoice_date,\n",
    "a.invoice_number,\n",
    "a.item_number,\n",
    "a.item,\n",
    "a.line_number,\n",
    "a.list_price,\n",
    "a.order_number,\n",
    "a.promised_delivery_date,\n",
    "a.sales_amount,\n",
    "a.sales_amount_based_on_list_price,\n",
    "a.sales_cost_amount,\n",
    "a.sales_margin_amount,\n",
    "a.sales_price,\n",
    "a.sales_quantity,\n",
    "a.sales_rep,\n",
    "a.u_m,\n",
    "\n",
    "b.business_family,\n",
    "b.business_unit,\n",
    "b.customer,\n",
    "b.customer_type,\n",
    "b.line_of_business,\n",
    "b.phone,\n",
    "b.regional_sales_mgr,\n",
    "b.search_type,\n",
    "b.dt_foto,\n",
    "\n",
    "c.division,\n",
    "c.division_name,\n",
    "\n",
    "d.region_code,\n",
    "d.region_name,\n",
    "\n",
    "e.address_number,\n",
    "e.city,\n",
    "e.country,\n",
    "e.customer_address_1,\n",
    "e.customer_address_2,\n",
    "e.customer_address_3,\n",
    "e.customer_address_4,\n",
    "e.state,\n",
    "e.zip_code\n",
    "\n",
    "from vendas a\n",
    "left join clientes b\n",
    "on a.customerkey = b.customerkey\n",
    "\n",
    "left join divisao c\n",
    "on b.division = c.division\n",
    "\n",
    "left join regiao d\n",
    "on b.region_code = d.region_code\n",
    "\n",
    "left join endereco e\n",
    "on b.address_number = e.address_number\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da STAGE\n",
    "df_stage = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação dos Campos Calendario\n",
    "df_stage = (df_stage\n",
    "            .withColumn('Ano', year(df_stage.invoice_date))\n",
    "            .withColumn('Mes', month(df_stage.invoice_date))\n",
    "            .withColumn('Dia', dayofmonth(df_stage.invoice_date))\n",
    "            .withColumn('Trimestre', quarter(df_stage.invoice_date))\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação das Chaves do Modelo\n",
    "\n",
    "df_stage = df_stage.withColumn(\"DW_CLIENTE\", sha2(concat_ws(\"\", df_stage.customer, df_stage.customerkey, df_stage.customer_type), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_TEMPO\", sha2(concat_ws(\"\", df_stage.invoice_date, df_stage.Ano, df_stage.Mes, df_stage.Dia), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_LOCALIDADE\", sha2(concat_ws(\"\", df_stage.address_number, df_stage.city, df_stage.state, df_stage.country, df_stage.region_name), 256))\n",
    "\n",
    "df_stage.createOrReplaceTempView('stage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a dimensão Cliente\n",
    "\n",
    "dim_cliente = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_CLIENTE,\n",
    "        customerkey,\n",
    "        address_number,\n",
    "        business_family,\n",
    "        business_unit,\n",
    "        customer,\n",
    "        customer_type,\n",
    "        division,\n",
    "        line_of_business,\n",
    "        phone,\n",
    "        region_code,\n",
    "        regional_sales_mgr,\n",
    "        search_type\n",
    "    FROM stage    \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a dimensão Tempo\n",
    "\n",
    "dim_tempo = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_TEMPO,\n",
    "        invoice_date,\n",
    "        Ano,\n",
    "        Mes,\n",
    "        Dia\n",
    "    FROM stage   \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a dimensão Localidade\n",
    "\n",
    "dim_localidade = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_LOCALIDADE,\n",
    "        address_number,\n",
    "        city,\n",
    "        state,\n",
    "        country,\n",
    "        region_name\n",
    "    FROM stage   \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a fato Vendas\n",
    "\n",
    "ft_vendas = spark.sql('''\n",
    "    SELECT \n",
    "        DW_CLIENTE,\n",
    "        DW_TEMPO,\n",
    "        DW_LOCALIDADE,\n",
    "        sum(sales_amount) as vl_total\n",
    "    FROM stage\n",
    "    group by \n",
    "        DW_CLIENTE,\n",
    "        DW_TEMPO,\n",
    "        DW_LOCALIDADE\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_cliente/part-* /input/desafio_curso/gold/dim_cliente.csv\n",
      "hdfs dfs -get /datalake/gold/dim_tempo/part-* /input/desafio_curso/gold/dim_tempo.csv\n",
      "hdfs dfs -get /datalake/gold/dim_localidade/part-* /input/desafio_curso/gold/dim_localidade.csv\n",
      "hdfs dfs -get /datalake/gold/ft_vendas/part-* /input/desafio_curso/gold/ft_vendas.csv\n"
     ]
    }
   ],
   "source": [
    "# função para salvar os dados\n",
    "def salvar_df(df, file):\n",
    "    output = \"/input/desafio_curso/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* /input/desafio_curso/gold/\"+file+\".csv\"\n",
    "    print(rename)\n",
    "    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "\n",
    "    os.system(erase)\n",
    "    os.system(rename)\n",
    "\n",
    "salvar_df(dim_cliente, 'dim_cliente')\n",
    "salvar_df(dim_tempo, 'dim_tempo')\n",
    "salvar_df(dim_localidade, 'dim_localidade')\n",
    "salvar_df(ft_vendas, 'ft_vendas')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
